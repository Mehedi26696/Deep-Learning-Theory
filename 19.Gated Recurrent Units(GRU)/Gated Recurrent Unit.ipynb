{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cd1e50c-00fb-432a-ad90-2017027a8b8f",
   "metadata": {},
   "source": [
    "<center><h1 style=\"color:green\">Gated Recurrent Unit (GRU)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0079011-980e-497d-aa96-b37483760416",
   "metadata": {},
   "source": [
    "## üîπ Introduction\n",
    "The Gated Recurrent Unit (GRU) is a type of Recurrent Neural Network (RNN) architecture designed to address the vanishing gradient problem. It is similar to Long Short-Term Memory (LSTM) but has a **simpler structure** with fewer parameters, making it computationally efficient.\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ GRU Architecture\n",
    "A GRU unit consists of:\n",
    "\n",
    "### **üîπ Hidden State ($h_t$)**\n",
    "- Acts as the memory that carries information across time steps.\n",
    "- Updated using two gates: **Reset Gate** and **Update Gate**.\n",
    "\n",
    "### **üîπ Gates in GRU**\n",
    "1. **Reset Gate ($r_t$)**\n",
    "   - Determines how much of the previous hidden state should be forgotten.\n",
    "   - Formula:\n",
    "     \n",
    "     $ r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r) $\n",
    "     \n",
    "   - Uses a sigmoid activation function.\n",
    "\n",
    "2. **Update Gate ($z_t$)**\n",
    "   - Controls how much of the previous hidden state should be carried forward.\n",
    "   - Formula:\n",
    "     \n",
    "     $ z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z) $\n",
    "     \n",
    "3. **Candidate Activation ($\\tilde{h}_t$)**\n",
    "   - Represents a candidate for the new hidden state.\n",
    "   - Formula:\n",
    "     \n",
    "     $ \\tilde{h}_t = \\tanh(W_h \\cdot [r_t \\cdot h_{t-1}, x_t] + b_h) $\n",
    "     \n",
    "4. **Final Hidden State ($h_t$)**\n",
    "   - The final hidden state is a combination of the previous state and the candidate activation.\n",
    "   - Formula:\n",
    "     \n",
    "     $ h_t = (1 - z_t) \\cdot \\tilde{h}_t + z_t \\cdot h_{t-1} $\n",
    "     \n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Why GRU Over Traditional RNNs?\n",
    "### ‚úÖ **Simpler than LSTM**\n",
    "- GRU has only **two gates** instead of three, making it computationally more efficient.\n",
    "\n",
    "### ‚úÖ **Solves the Vanishing Gradient Problem**\n",
    "- The gating mechanism helps in **long-term dependency learning**.\n",
    "\n",
    "### ‚úÖ **Efficiently Captures Sequential Data Patterns**\n",
    "- Works well for **speech recognition, language modeling, and time-series forecasting**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Applications of GRU\n",
    "- ‚úî **Text and Speech Processing** (e.g., Machine Translation, Chatbots)\n",
    "- ‚úî **Time-Series Forecasting**\n",
    "- ‚úî **Stock Market Prediction**\n",
    "- ‚úî **Anomaly Detection**\n",
    "- ‚úî **Gesture and Handwriting Recognition**\n",
    "\n",
    "---\n",
    "\n",
    "## üî• Conclusion\n",
    "GRUs are a simplified alternative to LSTMs, providing similar advantages in handling long-term dependencies but with fewer parameters. Their efficiency and effectiveness make them widely used in NLP, speech processing, and time-series tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa64b7b-09f3-4ccc-9401-2cbe118fb440",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
