{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78fc5feb-33bc-40ff-973e-13748f5a5e30",
   "metadata": {},
   "source": [
    "# 📚 **Activation Functions in Neural Networks**\n",
    "\n",
    "## 📌 **1. Introduction**\n",
    "Activation functions introduce **non-linearity** into neural networks, enabling them to learn and represent complex relationships in data. Without activation functions, a neural network behaves like a linear regression model, no matter how many layers it has.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ **2. Key Properties of Activation Functions**\n",
    "- **Non-linearity:** Allows the network to model complex patterns.\n",
    "- **Differentiability:** Essential for gradient-based optimization.\n",
    "- **Range:** Output range (e.g., [0,1] or [-1,1]).\n",
    "- **Monotonicity:** Helps in stable gradient descent.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 **3. Common Activation Functions**\n",
    "\n",
    "### 🔹 **3.1 Sigmoid Function**\n",
    "- **Formula:** $$ f(x) = \\frac{1}{1 + e^{-x}} $$\n",
    "- **Range:** (0,1)\n",
    "- **Pros:** Smooth gradient, probability interpretation.\n",
    "- **Cons:** Vanishing gradient problem.\n",
    "\n",
    "### 🔹 **3.2 Tanh Function**\n",
    "- **Formula:** $$ f(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $$\n",
    "- **Range:** (-1,1)\n",
    "- **Pros:** Zero-centered outputs.\n",
    "- **Cons:** Vanishing gradient problem.\n",
    "\n",
    "### 🔹 **3.3 ReLU (Rectified Linear Unit)**\n",
    "- **Formula:** $$ f(x) = \\max(0, x) $$\n",
    "- **Range:** [0, ∞)\n",
    "- **Pros:** Computationally efficient, mitigates vanishing gradient.\n",
    "- **Cons:** Dying ReLU problem (neurons stuck at 0).\n",
    "\n",
    "### 🔹 **3.4 Leaky ReLU**\n",
    "- **Formula:** $$ f(x) = \\max(\\alpha x, x) $$ where $\\alpha$ is a small positive constant.\n",
    "- **Range:** (-∞, ∞)\n",
    "- **Pros:** Solves dying ReLU problem.\n",
    "\n",
    "### 🔹 **3.5 Softmax Function**\n",
    "- **Formula:** $$ f(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}} $$\n",
    "- **Range:** (0,1) (sum equals 1)\n",
    "- **Use Case:** Multi-class classification.\n",
    "\n",
    "### 🔹 **3.6 Softplus Function**\n",
    "- **Formula:** $$ f(x) = \\ln(1 + e^x) $$\n",
    "- **Range:** (0, ∞)\n",
    "- **Pros:** Smooth approximation of ReLU, differentiable everywhere.\n",
    "- **Cons:** Computationally expensive compared to ReLU.\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 **4. Comparison Table**\n",
    "\n",
    "| Function     | Formula              | Range       | Pros                | Cons               |\n",
    "|--------------|-----------------------|------------|----------------------|---------------------|\n",
    "| Sigmoid      | $$ \\frac{1}{1+e^{-x}} $$ | (0,1)      | Probability output  | Vanishing gradient |\n",
    "| Tanh         | $$ \\tanh(x) $$       | (-1,1)      | Zero-centered       | Vanishing gradient |\n",
    "| ReLU         | $$ \\max(0, x) $$    | [0,∞)       | Fast computation    | Dying ReLU         |\n",
    "| Leaky ReLU   | $$ \\max(\\alpha x, x) $$ | (-∞,∞)   | Prevents dying ReLU | Complexity         |\n",
    "| Softmax      | $$ \\frac{e^{x_i}}{\\sum_j e^{x_j}} $$ | (0,1) | Probabilities | Computational cost |\n",
    "| Softplus     | $$ \\ln(1 + e^x) $$  | (0,∞)       | Smooth, differentiable | Computational cost |\n",
    "\n",
    "---\n",
    "\n",
    "## 📝 **5. Choosing the Right Activation Function**\n",
    "- **Sigmoid/Tanh:** Use in output layers for binary classification.\n",
    "- **ReLU/Leaky ReLU:** Common in hidden layers.\n",
    "- **Softmax:** Use in output layer for multi-class classification.\n",
    "- **Softplus:** Alternative to ReLU when smoothness is needed.\n",
    "\n",
    "---\n",
    "\n",
    "## 📦 **6. Summary**\n",
    "Activation functions play a critical role in determining the performance and efficiency of neural networks. Choosing the right function depends on the specific use case and architecture of the network.\n",
    "\n",
    "---\n",
    "\n",
    "**🔗 Further Reading:**\n",
    "- Neural Networks and Deep Learning by Michael Nielsen\n",
    "- Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow by Aurélien Géron\n",
    "\n",
    "---\n",
    "\n",
    "> *\"The activation function is the heart of a neural network.\"* 🚀\n",
    "\n",
    "---\n",
    "\n",
    "✅ **End of Notes**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f236e4-babb-4dc8-aa4a-f06fd89e1c88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
