{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ac0ce9e-2d0c-4ba0-af81-0256cdca0f08",
   "metadata": {},
   "source": [
    "<center><h1 style=\"color:green\">Long Short-Term Memory (LSTM)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aa2520-9b2b-440b-b21f-4f887af94405",
   "metadata": {},
   "source": [
    "## üîπ Introduction\n",
    "Long Short-Term Memory (LSTM) is a type of Recurrent Neural Network (RNN) designed to handle long-term dependencies in sequential data. It overcomes the vanishing gradient problem of traditional RNNs by using a **memory cell** and specialized gates.\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ LSTM Architecture\n",
    "An LSTM unit consists of:\n",
    "\n",
    "### **üîπ Cell State ($C_t$)**\n",
    "- Acts as a memory that carries information across time steps.\n",
    "- Controlled by three gates.\n",
    "\n",
    "### **üîπ Gates in LSTM**\n",
    "1. **Forget Gate ($f_t$)**\n",
    "   - Decides what information from the previous cell state should be discarded.\n",
    "   - Formula:\n",
    "     \n",
    "     \n",
    "     \n",
    "     $ f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f) $\n",
    "     \n",
    "   - Uses a sigmoid activation function.\n",
    "\n",
    "2. **Input Gate ($i_t$)**\n",
    "   - Determines what new information to store in the cell state.\n",
    "   - Formula:\n",
    "     \n",
    "     $ i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i) $\n",
    "     \n",
    "   - A candidate memory cell update:\n",
    "     \n",
    "     $ \\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C) $\n",
    "     \n",
    "   - The new cell state:\n",
    "     \n",
    "     $ C_t = f_t \\cdot C_{t-1} + i_t \\cdot \\tilde{C}_t $\n",
    "     \n",
    "\n",
    "3. **Output Gate ($o_t$)**\n",
    "   - Determines the final output at the current time step.\n",
    "   - Formula:\n",
    "     \n",
    "     $ o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o) $\n",
    "     $ h_t = o_t \\cdot \\tanh(C_t) $\n",
    "     \n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Why LSTM Over Traditional RNNs?\n",
    "### ‚úÖ **Solves the Vanishing Gradient Problem**\n",
    "- The **cell state** allows LSTM to carry information for long time steps.\n",
    "\n",
    "### ‚úÖ **Efficiently Captures Long-Term Dependencies**\n",
    "- Unlike standard RNNs, LSTMs selectively retain and forget information.\n",
    "\n",
    "### ‚úÖ **Better Performance in Sequential Data Tasks**\n",
    "- Works well for **speech recognition, machine translation, and time-series forecasting**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Applications of LSTM\n",
    "- ‚úî **Text Generation** (e.g., Chatbots, Language Modeling)\n",
    "- ‚úî **Speech Recognition**\n",
    "- ‚úî **Machine Translation**\n",
    "- ‚úî **Stock Price Prediction**\n",
    "- ‚úî **Time-Series Forecasting**\n",
    "- ‚úî **Anomaly Detection**\n",
    "\n",
    "---\n",
    "\n",
    "## üî• Conclusion\n",
    "LSTM is a powerful enhancement over traditional RNNs, allowing networks to remember important information over long sequences. Its gating mechanism ensures efficient learning in sequential tasks, making it widely used in NLP, speech processing, and forecasting applications.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
