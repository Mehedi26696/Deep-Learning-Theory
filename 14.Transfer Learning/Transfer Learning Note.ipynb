{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2134e043-4110-44ec-b156-47862911a0a5",
   "metadata": {},
   "source": [
    "## Transfer Learning in Deep Learning\n",
    "\n",
    "### üìå Introduction\n",
    "Transfer Learning is a machine learning technique where a model trained on one task is reused or fine-tuned for a different but related task. Instead of training a model from scratch, we leverage knowledge from pre-trained models, which helps in achieving better performance with less data and computation.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Why Transfer Learning?\n",
    "‚úÖ Reduces training time by leveraging pre-trained models.  \n",
    "‚úÖ Helps when there is limited data available.  \n",
    "‚úÖ Avoids overfitting by using features learned from a larger dataset.  \n",
    "‚úÖ Achieves high accuracy with minimal resources.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Types of Transfer Learning\n",
    "1. **Feature Extraction**: Using the pre-trained model as a fixed feature extractor. \n",
    "   - Remove the last classification layer.\n",
    "   - Extract features from the earlier layers.\n",
    "   - Train a new classifier on top of the extracted features.\n",
    "\n",
    "2. **Fine-tuning**: Unfreezing some of the layers of the pre-trained model and retraining with a smaller learning rate.\n",
    "   - Typically, earlier layers are frozen as they contain general features.\n",
    "   - Later layers are fine-tuned to adapt to the new task.\n",
    "\n",
    "---\n",
    "\n",
    "### üî• Commonly Used Pre-trained Models\n",
    "| Model | Architecture | Pre-trained on |\n",
    "|--------|----------------|----------------|\n",
    "| VGG16 | Convolutional Neural Network (CNN) | ImageNet |\n",
    "| ResNet | Deep Residual Network | ImageNet |\n",
    "| Inception | GoogleNet | ImageNet |\n",
    "| MobileNet | Lightweight CNN | ImageNet |\n",
    "| BERT | Transformer-based NLP model | Text Corpus |\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Steps for Applying Transfer Learning\n",
    "#### 1Ô∏è‚É£ Load a Pre-trained Model\n",
    "```python\n",
    "from tensorflow.keras.applications import VGG16\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "```\n",
    "\n",
    "#### 2Ô∏è‚É£ Freeze Some Layers (Feature Extraction)\n",
    "```python\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False  # Freeze all layers\n",
    "```\n",
    "\n",
    "#### 3Ô∏è‚É£ Add Custom Layers for New Task\n",
    "```python\n",
    "from tensorflow.keras import models, layers\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')  # Change the number of classes as needed\n",
    "])\n",
    "```\n",
    "\n",
    "#### 4Ô∏è‚É£ Compile and Train the Model\n",
    "```python\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_data, epochs=10, validation_data=val_data)\n",
    "```\n",
    "\n",
    "#### 5Ô∏è‚É£ Fine-Tune the Model (Optional)\n",
    "```python\n",
    "for layer in base_model.layers[-4:]:  # Unfreeze last 4 layers\n",
    "    layer.trainable = True\n",
    "```\n",
    "```python\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_data, epochs=5, validation_data=val_data)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Applications of Transfer Learning\n",
    "‚úÖ Image Classification (e.g., Medical Imaging, Object Detection)  \n",
    "‚úÖ Natural Language Processing (e.g., Sentiment Analysis, Chatbots)  \n",
    "‚úÖ Speech Recognition (e.g., Voice Assistants)  \n",
    "‚úÖ Video Analysis (e.g., Action Recognition, Video Captioning)  \n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Key Takeaways\n",
    "üîπ Transfer Learning helps leverage pre-trained models to solve new problems efficiently.  \n",
    "üîπ Feature extraction and fine-tuning are the two primary approaches.  \n",
    "üîπ Works well with limited data and reduces overfitting.  \n",
    "üîπ Widely used in computer vision and NLP tasks.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
