{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53274b29-c81d-4c05-b145-68b028e107bb",
   "metadata": {},
   "source": [
    "<center><h1 style=\"color:green\">Recurrent Neural Networks (RNN)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd550a9-58a8-4c1d-8d85-44afc255f7cd",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "Recurrent Neural Networks (RNNs) are a class of artificial neural networks designed for sequential data, where past information is used to inform future predictions. This document provides a detailed guide on RNNs using Jupyter Notebook Markdown syntax, including equations, code snippets, tables, and images.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Overview of RNNs\n",
    "### What is an RNN?\n",
    "Recurrent Neural Networks (RNNs) are a type of neural network where connections form cycles, allowing information to persist. This makes them well-suited for tasks involving sequential data such as time series forecasting, natural language processing, and speech recognition.\n",
    "\n",
    "### Key Features of RNNs:\n",
    "- **Handles sequential data** by maintaining hidden states.\n",
    "- **Uses shared weights** across time steps.\n",
    "- **Captures temporal dependencies** in data.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Mathematical Representation\n",
    "### RNN State Transition Equations\n",
    " \n",
    "$$ h_t = f(W_h h_{t-1} + W_x x_t + b) $$\n",
    " \n",
    "This equation describes the update of the hidden state $ h_t $ at time step $ t $:\n",
    "- $ W_h $: Weight matrix for the hidden state.\n",
    "- $ W_x $: Weight matrix for the input.\n",
    "- $ x_t $: Input at time step $ t $.\n",
    "- $ h_{t-1} $: Previous hidden state.\n",
    "- $ b $: Bias term.\n",
    "- $ f $: Activation function, typically `tanh` or `ReLU`.\n",
    "\n",
    "### Output Equation\n",
    " \n",
    "$$ o_t = g(W_o h_t + b_o) $$\n",
    " \n",
    "Where:\n",
    "- $ W_o $ and $ b_o $ are weight and bias for the output layer.\n",
    "- $ g $ is an activation function such as `softmax` for classification.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Architecture of RNN\n",
    "### Structure of a Simple RNN\n",
    " \n",
    "<img src=\"1.png\" width=\"500\">\n",
    " \n",
    "\n",
    "### Table Representation\n",
    " \n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| Input Layer | Accepts sequence data |\n",
    "| Hidden Layer | Stores memory across time steps |\n",
    "| Output Layer | Produces final predictions |\n",
    " \n",
    "Output:\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| Input Layer | Accepts sequence data |\n",
    "| Hidden Layer | Stores memory across time steps |\n",
    "| Output Layer | Produces final predictions |\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Python Implementation in Jupyter Notebook\n",
    "### Importing Libraries\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "```\n",
    "\n",
    "### Building an RNN Model\n",
    "```python\n",
    "model = Sequential([\n",
    "    SimpleRNN(50, activation='tanh', input_shape=(10, 5)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "- `SimpleRNN(50)`: 50 hidden units.\n",
    "- `input_shape=(10,5)`: 10 time steps, 5 features.\n",
    "- `Dense(1, activation='sigmoid')`: Output layer with a single neuron.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Applications of RNNs\n",
    "### Common Use Cases\n",
    "- **Natural Language Processing (NLP)**: Sentiment analysis, machine translation.\n",
    "- **Time Series Forecasting**: Stock price prediction, weather forecasting.\n",
    "- **Speech Recognition**: Voice-to-text applications.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Advantages & Disadvantages\n",
    "### Advantages:\n",
    "- Efficient handling of sequential data.\n",
    "- Shared weights reduce complexity.\n",
    "- Captures time-dependent patterns.\n",
    "\n",
    "### Disadvantages:\n",
    "- Vanishing gradient problem.\n",
    "- Difficulty in capturing long-term dependencies.\n",
    "- Computationally expensive for long sequences.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf7e779-1b17-47d1-bb62-b2cae736537a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
