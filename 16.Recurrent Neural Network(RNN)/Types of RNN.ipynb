{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e15b187-9ffb-4224-977b-5b26312ecf15",
   "metadata": {},
   "source": [
    "<center><h1 style=\"color:green\">Types of Recurrent Neural Networks (RNNs)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15441cf-dbfe-44cc-9352-d9f893f52d11",
   "metadata": {},
   "source": [
    "\n",
    "Recurrent Neural Networks (RNNs) are a type of neural network designed for sequential data, where past information influences future predictions. There are different types of RNNs, each suited for specific tasks.\n",
    "\n",
    "## 1Ô∏è‚É£ Vanilla RNN (Standard RNN)\n",
    "### **Structure:**\n",
    "- Basic RNN where the hidden state is passed from one time step to another.\n",
    "- Uses activation functions like Tanh or ReLU.\n",
    "\n",
    "### **Limitations:**\n",
    "- Suffers from **vanishing gradient** and **exploding gradient** problems, making it hard to learn long-term dependencies.\n",
    "\n",
    "### **Use Cases:**\n",
    "- Simple sequence prediction tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Long Short-Term Memory (LSTM)\n",
    "### **Structure:**\n",
    "- Introduced to overcome the vanishing gradient problem.\n",
    "- Consists of three gates:\n",
    "  - **Forget Gate**: Decides what information to discard.\n",
    "  - **Input Gate**: Determines what new information to store.\n",
    "  - **Output Gate**: Controls the final output.\n",
    "\n",
    "### **Advantages:**\n",
    "- Can capture long-term dependencies.\n",
    "- Solves vanishing gradient issues.\n",
    "\n",
    "### **Use Cases:**\n",
    "- Text generation, speech recognition, and time-series forecasting.\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Gated Recurrent Unit (GRU)\n",
    "### **Structure:**\n",
    "- A simplified version of LSTM with only two gates:\n",
    "  - **Reset Gate**: Controls past memory influence.\n",
    "  - **Update Gate**: Determines new information to store.\n",
    "\n",
    "### **Advantages:**\n",
    "- Computationally more efficient than LSTMs.\n",
    "- Performs well on smaller datasets.\n",
    "\n",
    "### **Use Cases:**\n",
    "- Chatbots, real-time speech recognition, and machine translation.\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Bidirectional RNN (Bi-RNN)\n",
    "### **Structure:**\n",
    "- Processes the sequence **in both forward and backward directions**.\n",
    "- Utilizes two hidden layers instead of one.\n",
    "\n",
    "### **Advantages:**\n",
    "- More context-aware, as it learns from both past and future sequences.\n",
    "\n",
    "### **Use Cases:**\n",
    "- Named Entity Recognition (NER), speech recognition, and handwriting recognition.\n",
    "\n",
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Encoder-Decoder RNN\n",
    "### **Structure:**\n",
    "- An architecture used for sequence-to-sequence tasks.\n",
    "- **Encoder** processes the input sequence and produces a context vector.\n",
    "- **Decoder** generates the output sequence using the context vector.\n",
    "\n",
    "### **Advantages:**\n",
    "- Can handle variable-length input and output sequences.\n",
    "- Useful for complex language tasks.\n",
    "\n",
    "### **Use Cases:**\n",
    "- Machine translation, chatbot response generation, and text summarization.\n",
    "\n",
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ Attention-Based RNN\n",
    "### **Structure:**\n",
    "- Enhances the Encoder-Decoder model by focusing on relevant parts of the input sequence.\n",
    "- Uses an **attention mechanism** to assign weights to different input tokens.\n",
    "\n",
    "### **Advantages:**\n",
    "- Handles long sequences effectively.\n",
    "- Improves translation quality and sequence modeling.\n",
    "\n",
    "### **Use Cases:**\n",
    "- Neural Machine Translation (NMT), image captioning, and speech processing.\n",
    "\n",
    "---\n",
    "\n",
    "## üî• Conclusion\n",
    "Different RNN architectures are designed to handle different challenges in sequential data processing. While Vanilla RNNs suffer from memory limitations, LSTMs and GRUs provide better performance. Advanced models like Bi-RNN, Encoder-Decoder, and Attention-based RNNs enhance efficiency and accuracy for real-world applications.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
